{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a1f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 454 CSV files.\n",
      "[1/454] Processed AP001.csv (59150 rows)\n",
      "[2/454] Processed AP002.csv (51864 rows)\n",
      "[3/454] Processed AP003.csv (50400 rows)\n",
      "[4/454] Processed AP004.csv (48802 rows)\n",
      "[5/454] Processed AP005.csv (46880 rows)\n",
      "[6/454] Processed AP006.csv (5432 rows)\n",
      "[7/454] Processed AP007.csv (3127 rows)\n",
      "[8/454] Processed AP008.csv (3280 rows)\n",
      "[9/454] Processed AP009.csv (1689 rows)\n",
      "[10/454] Processed AP010.csv (1593 rows)\n",
      "[11/454] Processed AR001.csv (17655 rows)\n",
      "[12/454] Processed AS001.csv (35808 rows)\n",
      "[13/454] Processed AS002.csv (19764 rows)\n",
      "[14/454] Processed AS003.csv (4641 rows)\n",
      "[15/454] Processed AS004.csv (4400 rows)\n",
      "[16/454] Processed AS005.csv (3081 rows)\n",
      "[17/454] Processed AS006.csv (2462 rows)\n",
      "[18/454] Processed AS007.csv (2390 rows)\n",
      "[19/454] Processed AS008.csv (1881 rows)\n",
      "[20/454] Processed AS009.csv (744 rows)\n",
      "[21/454] Processed BR001.csv (94224 rows)\n",
      "[22/454] Processed BR002.csv (65736 rows)\n",
      "[23/454] Processed BR003.csv (62088 rows)\n",
      "[24/454] Processed BR004.csv (28639 rows)\n",
      "[25/454] Processed BR005.csv (28639 rows)\n",
      "[26/454] Processed BR006.csv (28639 rows)\n",
      "[27/454] Processed BR007.csv (27024 rows)\n",
      "[28/454] Processed BR008.csv (27024 rows)\n",
      "[29/454] Processed BR009.csv (27024 rows)\n",
      "[30/454] Processed BR010.csv (26959 rows)\n",
      "[31/454] Processed BR011.csv (22780 rows)\n",
      "[32/454] Processed BR012.csv (14725 rows)\n",
      "[33/454] Processed BR013.csv (14889 rows)\n",
      "[34/454] Processed BR014.csv (14938 rows)\n",
      "[35/454] Processed BR015.csv (14936 rows)\n",
      "[36/454] Processed BR016.csv (15156 rows)\n",
      "[37/454] Processed BR017.csv (14726 rows)\n",
      "[38/454] Processed BR018.csv (14725 rows)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Done! Fixed merged file saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Run this to rebuild Kaggle merged dataset\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m load_pm_pollutants_fixed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/kaggle_csvs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/kaggle_pm_merged_fixed.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 48\u001b[0m, in \u001b[0;36mload_pm_pollutants_fixed\u001b[1;34m(dir_path, out_file)\u001b[0m\n\u001b[0;32m     46\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(out_file, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Processed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m df\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3968\u001b[0m     path_or_buf,\n\u001b[0;32m   3969\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3970\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3971\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3972\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3973\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3974\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3975\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3976\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3977\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3978\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3979\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3980\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3981\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3982\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3983\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3984\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:270\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:275\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_body()\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:313\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_chunk(start_i, end_i)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:320\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    317\u001b[0m slicer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(start_i, end_i)\n\u001b[0;32m    318\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc[slicer]\n\u001b[1;32m--> 320\u001b[0m res \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_get_values_for_csv(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n\u001b[0;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(res\u001b[38;5;241m.\u001b[39m_iter_column_arrays())\n\u001b[0;32m    323\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_get_values_for_csv(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:1410\u001b[0m, in \u001b[0;36mDataFrame._get_values_for_csv\u001b[1;34m(self, float_format, date_format, decimal, na_rep, quoting)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_values_for_csv\u001b[39m(\n\u001b[0;32m   1401\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1402\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1409\u001b[0m     \u001b[38;5;66;03m# helper used by to_csv\u001b[39;00m\n\u001b[1;32m-> 1410\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mget_values_for_csv(\n\u001b[0;32m   1411\u001b[0m         float_format\u001b[38;5;241m=\u001b[39mfloat_format,\n\u001b[0;32m   1412\u001b[0m         date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   1413\u001b[0m         decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   1414\u001b[0m         na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[0;32m   1415\u001b[0m         quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   1416\u001b[0m     )\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;66;03m# error: Incompatible return value type (got \"DataFrame\", expected \"Self\")\u001b[39;00m\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(mgr, axes\u001b[38;5;241m=\u001b[39mmgr\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:466\u001b[0m, in \u001b[0;36mBaseBlockManager.get_values_for_csv\u001b[1;34m(self, float_format, date_format, decimal, na_rep, quoting)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_values_for_csv\u001b[39m(\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, float_format, date_format, decimal, na_rep: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m, quoting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    461\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m    462\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;124;03m    Convert values to native types (strings / python objects) that are used\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m    in formatting (repr / csv).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_values_for_csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    468\u001b[0m         na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[0;32m    469\u001b[0m         quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m    470\u001b[0m         float_format\u001b[38;5;241m=\u001b[39mfloat_format,\n\u001b[0;32m    471\u001b[0m         date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m    472\u001b[0m         decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m    473\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:780\u001b[0m, in \u001b[0;36mBlock.get_values_for_csv\u001b[1;34m(self, float_format, date_format, decimal, na_rep, quoting)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_values_for_csv\u001b[39m(\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, float_format, date_format, decimal, na_rep: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m, quoting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    778\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Block:\n\u001b[0;32m    779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"convert to our native types format\"\"\"\u001b[39;00m\n\u001b[1;32m--> 780\u001b[0m     result \u001b[38;5;241m=\u001b[39m get_values_for_csv(\n\u001b[0;32m    781\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m    782\u001b[0m         na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[0;32m    783\u001b[0m         quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m    784\u001b[0m         float_format\u001b[38;5;241m=\u001b[39mfloat_format,\n\u001b[0;32m    785\u001b[0m         date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m    786\u001b[0m         decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m    787\u001b[0m     )\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_block(result)\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7838\u001b[0m, in \u001b[0;36mget_values_for_csv\u001b[1;34m(values, date_format, na_rep, quoting, float_format, decimal)\u001b[0m\n\u001b[0;32m   7835\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   7836\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(values, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7838\u001b[0m values[mask] \u001b[38;5;241m=\u001b[39m na_rep\n\u001b[0;32m   7839\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   7840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def load_pm_pollutants_fixed(dir_path=\"data/kaggle_csvs\", out_file=\"data/kaggle_pm_merged_fixed.csv\"):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*.csv\"))\n",
    "    print(f\"Found {len(files)} CSV files.\")\n",
    "    os.makedirs(os.path.dirname(out_file), exist_ok=True)\n",
    "\n",
    "    first = True\n",
    "    for i, f in enumerate(files, 1):\n",
    "        try:\n",
    "            # Read column headers first\n",
    "            cols = [c.strip() for c in pd.read_csv(f, nrows=0).columns]\n",
    "\n",
    "            # Detect possible matching column names\n",
    "            mapping = {}\n",
    "            for c in cols:\n",
    "                c_lower = c.lower()\n",
    "                if \"date\" in c_lower or \"time\" in c_lower:\n",
    "                    mapping[c] = \"Timestamp\"\n",
    "                elif \"pm2\" in c_lower:\n",
    "                    mapping[c] = \"PM2.5\"\n",
    "                elif \"pm10\" in c_lower:\n",
    "                    mapping[c] = \"PM10\"\n",
    "                elif \"o3\" in c_lower or \"ozone\" in c_lower:\n",
    "                    mapping[c] = \"O3\"\n",
    "                elif \"co\" in c_lower and not \"co2\" in c_lower:\n",
    "                    mapping[c] = \"CO\"\n",
    "\n",
    "            if not mapping:\n",
    "                print(f\"⚠️ Skipped {f} (no relevant columns found)\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(f, usecols=mapping.keys(), encoding_errors=\"ignore\")\n",
    "            df.rename(columns=mapping, inplace=True)\n",
    "            df[\"StationFile\"] = os.path.basename(f)\n",
    "\n",
    "            # Make sure all columns exist\n",
    "            for col in [\"Timestamp\", \"PM2.5\", \"PM10\", \"O3\", \"CO\"]:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = None\n",
    "\n",
    "            if first:\n",
    "                df.to_csv(out_file, index=False)\n",
    "                first = False\n",
    "            else:\n",
    "                df.to_csv(out_file, mode=\"a\", index=False, header=False)\n",
    "\n",
    "            print(f\"[{i}/{len(files)}] Processed {os.path.basename(f)} ({len(df)} rows)\")\n",
    "            del df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Skipped {f}: {e}\")\n",
    "\n",
    "    print(f\"\\n✅ Done! Fixed merged file saved to: {out_file}\")\n",
    "\n",
    "# Run this to rebuild Kaggle merged dataset\n",
    "load_pm_pollutants_fixed(\"data/kaggle_csvs\", \"data/kaggle_pm_merged_fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468685aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Timestamp.1</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>StationFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 10:00:00</td>\n",
       "      <td>2016-07-01 11:00:00</td>\n",
       "      <td>10.67</td>\n",
       "      <td>39.00</td>\n",
       "      <td>0.48</td>\n",
       "      <td>14.5</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 11:00:00</td>\n",
       "      <td>2016-07-01 12:00:00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>39.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>15.0</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 12:00:00</td>\n",
       "      <td>2016-07-01 13:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01 13:00:00</td>\n",
       "      <td>2016-07-01 14:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01 14:00:00</td>\n",
       "      <td>2016-07-01 15:00:00</td>\n",
       "      <td>20.50</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.5</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-07-01 15:00:00</td>\n",
       "      <td>2016-07-01 16:00:00</td>\n",
       "      <td>15.25</td>\n",
       "      <td>59.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>6.6</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-07-01 16:00:00</td>\n",
       "      <td>2016-07-01 17:00:00</td>\n",
       "      <td>11.67</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>17.43</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-07-01 17:00:00</td>\n",
       "      <td>2016-07-01 18:00:00</td>\n",
       "      <td>11.75</td>\n",
       "      <td>57.50</td>\n",
       "      <td>0.44</td>\n",
       "      <td>19.98</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-07-01 18:00:00</td>\n",
       "      <td>2016-07-01 19:00:00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>57.75</td>\n",
       "      <td>0.45</td>\n",
       "      <td>12.2</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-07-01 19:00:00</td>\n",
       "      <td>2016-07-01 20:00:00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>63.00</td>\n",
       "      <td>0.41</td>\n",
       "      <td>17.5</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Timestamp          Timestamp.1  PM2.5   PM10    CO     O3  \\\n",
       "0  2016-07-01 10:00:00  2016-07-01 11:00:00  10.67  39.00  0.48   14.5   \n",
       "1  2016-07-01 11:00:00  2016-07-01 12:00:00   2.00  39.00  0.49   15.0   \n",
       "2  2016-07-01 12:00:00  2016-07-01 13:00:00    NaN    NaN   NaN    NaN   \n",
       "3  2016-07-01 13:00:00  2016-07-01 14:00:00    NaN    NaN   NaN    NaN   \n",
       "4  2016-07-01 14:00:00  2016-07-01 15:00:00  20.50  50.00  0.47   10.5   \n",
       "5  2016-07-01 15:00:00  2016-07-01 16:00:00  15.25  59.50  0.51    6.6   \n",
       "6  2016-07-01 16:00:00  2016-07-01 17:00:00  11.67  60.00  0.46  17.43   \n",
       "7  2016-07-01 17:00:00  2016-07-01 18:00:00  11.75  57.50  0.44  19.98   \n",
       "8  2016-07-01 18:00:00  2016-07-01 19:00:00  18.00  57.75  0.45   12.2   \n",
       "9  2016-07-01 19:00:00  2016-07-01 20:00:00  12.00  63.00  0.41   17.5   \n",
       "\n",
       "  StationFile  \n",
       "0   AP001.csv  \n",
       "1   AP001.csv  \n",
       "2   AP001.csv  \n",
       "3   AP001.csv  \n",
       "4   AP001.csv  \n",
       "5   AP001.csv  \n",
       "6   AP001.csv  \n",
       "7   AP001.csv  \n",
       "8   AP001.csv  \n",
       "9   AP001.csv  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_kaggle = pd.read_csv(\"data/kaggle_pm_merged_fixed.csv\", on_bad_lines=\"skip\", low_memory=False)\n",
    "merged_kaggle.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7c207be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined dataset shape: (14313933, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 10:00:00</td>\n",
       "      <td>10.67</td>\n",
       "      <td>39.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.48</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 11:00:00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01 13:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01 14:00:00</td>\n",
       "      <td>20.50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.47</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp  PM2.5  PM10    O3    CO         Source\n",
       "0 2016-07-01 10:00:00  10.67  39.0  14.5  0.48  KaggleStation\n",
       "1 2016-07-01 11:00:00   2.00  39.0  15.0  0.49  KaggleStation\n",
       "2 2016-07-01 12:00:00    NaN   NaN   NaN   NaN  KaggleStation\n",
       "3 2016-07-01 13:00:00    NaN   NaN   NaN   NaN  KaggleStation\n",
       "4 2016-07-01 14:00:00  20.50  50.0  10.5  0.47  KaggleStation"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_df = pd.read_csv(\"data/cities_combined.csv\")\n",
    "\n",
    "# Make sure Timestamp format matches\n",
    "for df in [merged_kaggle, city_df]:\n",
    "    for col in df.columns:\n",
    "        if \"time\" in col.lower() or \"date\" in col.lower():\n",
    "            df.rename(columns={col: \"Timestamp\"}, inplace=True)\n",
    "            break\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "merged_kaggle[\"Source\"] = \"KaggleStation\"\n",
    "city_df[\"Source\"] = \"CityCombined\"\n",
    "\n",
    "keep_cols = [\"Timestamp\", \"PM2.5\", \"PM10\", \"O3\", \"CO\", \"Source\"]\n",
    "for col in keep_cols:\n",
    "    if col not in city_df.columns:\n",
    "        city_df[col] = None\n",
    "\n",
    "combined_df = pd.concat([merged_kaggle[keep_cols], city_df[keep_cols]], ignore_index=True)\n",
    "print(\"✅ Combined dataset shape:\", combined_df.shape)\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7e96f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle dataset shape: (907914, 7)\n",
      "City dataset shape: (18270, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Timestamp.1</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>StationFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 10:00:00</td>\n",
       "      <td>2016-07-01 11:00:00</td>\n",
       "      <td>10.67</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>14.5</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 11:00:00</td>\n",
       "      <td>2016-07-01 12:00:00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>15.0</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 12:00:00</td>\n",
       "      <td>2016-07-01 13:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Timestamp          Timestamp.1  PM2.5  PM10    CO    O3  \\\n",
       "0  2016-07-01 10:00:00  2016-07-01 11:00:00  10.67  39.0  0.48  14.5   \n",
       "1  2016-07-01 11:00:00  2016-07-01 12:00:00   2.00  39.0  0.49  15.0   \n",
       "2  2016-07-01 12:00:00  2016-07-01 13:00:00    NaN   NaN   NaN   NaN   \n",
       "\n",
       "  StationFile  \n",
       "0   AP001.csv  \n",
       "1   AP001.csv  \n",
       "2   AP001.csv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Location</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NH3</th>\n",
       "      <th>SO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-01-2020</td>\n",
       "      <td>Bengaluru - Silk Board</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02-01-2020</td>\n",
       "      <td>Bengaluru - Silk Board</td>\n",
       "      <td>43.67</td>\n",
       "      <td>134.00</td>\n",
       "      <td>20.28</td>\n",
       "      <td>10.98</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.91</td>\n",
       "      <td>21.82</td>\n",
       "      <td>Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03-01-2020</td>\n",
       "      <td>Bengaluru - Silk Board</td>\n",
       "      <td>30.58</td>\n",
       "      <td>74.42</td>\n",
       "      <td>15.17</td>\n",
       "      <td>12.10</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.96</td>\n",
       "      <td>23.31</td>\n",
       "      <td>Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Timestamp                Location  PM2.5    PM10    NO2    NH3   SO2  \\\n",
       "0  01-01-2020  Bengaluru - Silk Board    NaN     NaN    NaN    NaN   NaN   \n",
       "1  02-01-2020  Bengaluru - Silk Board  43.67  134.00  20.28  10.98  3.41   \n",
       "2  03-01-2020  Bengaluru - Silk Board  30.58   74.42  15.17  12.10  3.27   \n",
       "\n",
       "     CO     O3       City  \n",
       "0   NaN    NaN  Bengaluru  \n",
       "1  0.91  21.82  Bengaluru  \n",
       "2  0.96  23.31  Bengaluru  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new Kaggle dataset (the fixed one)\n",
    "kaggle_df = pd.read_csv(\"data/kaggle_pm_merged_fixed.csv\", on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "# Load your Combined City Pollution dataset\n",
    "city_df = pd.read_csv(\"data/cities_combined.csv\")\n",
    "\n",
    "print(\"Kaggle dataset shape:\", kaggle_df.shape)\n",
    "print(\"City dataset shape:\", city_df.shape)\n",
    "\n",
    "# Show first few rows from both\n",
    "display(kaggle_df.head(3))\n",
    "display(city_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16fd0286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined dataset shape: (915114, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 10:00:00</td>\n",
       "      <td>10.67</td>\n",
       "      <td>39.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.48</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 11:00:00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01 13:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01 14:00:00</td>\n",
       "      <td>20.50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.47</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp  PM2.5  PM10    O3    CO         Source\n",
       "0 2016-07-01 10:00:00  10.67  39.0  14.5  0.48  KaggleStation\n",
       "1 2016-07-01 11:00:00   2.00  39.0  15.0  0.49  KaggleStation\n",
       "2 2016-07-01 12:00:00    NaN   NaN   NaN   NaN  KaggleStation\n",
       "3 2016-07-01 13:00:00    NaN   NaN   NaN   NaN  KaggleStation\n",
       "4 2016-07-01 14:00:00  20.50  50.0  10.5  0.47  KaggleStation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize timestamp names\n",
    "for df in [kaggle_df, city_df]:\n",
    "    for col in df.columns:\n",
    "        if \"time\" in col.lower() or \"date\" in col.lower():\n",
    "            df.rename(columns={col: \"Timestamp\"}, inplace=True)\n",
    "            break\n",
    "\n",
    "# Force datetime conversion + DROP invalid timestamps\n",
    "for df in [kaggle_df, city_df]:\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "    df.dropna(subset=[\"Timestamp\"], inplace=True)\n",
    "\n",
    "# Add source column\n",
    "kaggle_df[\"Source\"] = \"KaggleStation\"\n",
    "city_df[\"Source\"] = \"CityCombined\"\n",
    "\n",
    "# Keep only the matching pollutant columns\n",
    "keep_cols = [\"Timestamp\", \"PM2.5\", \"PM10\", \"O3\", \"CO\", \"Source\"]\n",
    "\n",
    "# Add missing columns\n",
    "for col in keep_cols:\n",
    "    if col not in city_df.columns:\n",
    "        city_df[col] = None\n",
    "\n",
    "# Merge both datasets\n",
    "combined_df = pd.concat(\n",
    "    [kaggle_df[keep_cols], city_df[keep_cols]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(\"✅ Combined dataset shape:\", combined_df.shape)\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70e7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp      11070\n",
      "PM2.5        3829962\n",
      "PM10         4423580\n",
      "O3           3294769\n",
      "CO           3267773\n",
      "Source             0\n",
      "dtype: int64\n",
      "✅ After cleaning: (12144881, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 10:00:00</td>\n",
       "      <td>10.67</td>\n",
       "      <td>39.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.48</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 11:00:00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01 14:00:00</td>\n",
       "      <td>20.50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.47</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-07-01 15:00:00</td>\n",
       "      <td>15.25</td>\n",
       "      <td>59.5</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.51</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-07-01 16:00:00</td>\n",
       "      <td>11.67</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17.43</td>\n",
       "      <td>0.46</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp  PM2.5  PM10     O3    CO         Source\n",
       "0 2016-07-01 10:00:00  10.67  39.0   14.5  0.48  KaggleStation\n",
       "1 2016-07-01 11:00:00   2.00  39.0   15.0  0.49  KaggleStation\n",
       "4 2016-07-01 14:00:00  20.50  50.0   10.5  0.47  KaggleStation\n",
       "5 2016-07-01 15:00:00  15.25  59.5    6.6  0.51  KaggleStation\n",
       "6 2016-07-01 16:00:00  11.67  60.0  17.43  0.46  KaggleStation"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values\n",
    "print(combined_df.isna().sum())\n",
    "\n",
    "# Drop rows with completely empty pollutant data\n",
    "combined_df = combined_df.dropna(subset=[\"PM2.5\", \"PM10\", \"O3\", \"CO\"], how=\"all\")\n",
    "\n",
    "# Fill remaining missing values with median (optional)\n",
    "combined_df[[\"PM2.5\", \"PM10\", \"O3\", \"CO\"]] = combined_df[[\"PM2.5\", \"PM10\", \"O3\", \"CO\"]].fillna(\n",
    "    combined_df.median(numeric_only=True)\n",
    ")\n",
    "\n",
    "print(\"✅ After cleaning:\", combined_df.shape)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d7631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved clean dataset: data/master_airquality_clean.csv\n"
     ]
    }
   ],
   "source": [
    "combined_df.to_csv(\"data/master_airquality_clean.csv\", index=False)\n",
    "print(\"✅ Saved clean dataset: data/master_airquality_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b0ed5",
   "metadata": {},
   "source": [
    "#NOTHING IS IMPORTANT AFTER THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a894b1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in d:\\anaconda3\\lib\\site-packages (24.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "D:\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting pip\n",
      "  Using cached pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "Requirement already satisfied: pandas in d:\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in d:\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: xgboost in d:\\anaconda3\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: joblib in d:\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in d:\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in d:\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: aif360 in d:\\anaconda3\\lib\\site-packages (0.6.1)\n",
      "Collecting metaflow\n",
      "  Using cached metaflow-2.19.7-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting wandb\n",
      "  Using cached wandb-0.23.0-py3-none-win_amd64.whl.metadata (12 kB)\n",
      "Collecting bentoml\n",
      "  Using cached bentoml-1.4.28-py3-none-any.whl.metadata (16 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement whylabs-sdk (from versions: none)\n",
      "ERROR: No matching distribution found for whylabs-sdk\n"
     ]
    }
   ],
   "source": [
    "# pip install (run in a notebook cell with `!` or in terminal)\n",
    "!pip install --upgrade pip\n",
    "!pip install pandas numpy scikit-learn xgboost joblib matplotlib seaborn\n",
    "!pip install aif360 metaflow wandb bentoml whylabs-sdk fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36336576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp-like columns found: ['Timestamp']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vineet Raj\\AppData\\Local\\Temp\\ipykernel_23544\\2278563494.py:25: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce', dayfirst=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first parse, NaT fraction = 0.608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vineet Raj\\AppData\\Local\\Temp\\ipykernel_23544\\2278563494.py:33: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col].astype(str), errors='coerce', dayfirst=False, infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After second parse (dayfirst=False), NaT fraction = 0.608\n",
      "Dropped 7378375 rows with unparseable Timestamp\n",
      "Dataset now has shape: (4766506, 9)\n",
      "Timestamp min/max: 2010-01-01 00:00:00 / 2024-12-12 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>Source</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-07 10:00:00</td>\n",
       "      <td>10.67</td>\n",
       "      <td>39.0</td>\n",
       "      <td>14.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>KaggleStation</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-07 11:00:00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>39.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>KaggleStation</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-07 14:00:00</td>\n",
       "      <td>20.50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>10.50</td>\n",
       "      <td>0.47</td>\n",
       "      <td>KaggleStation</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-07 15:00:00</td>\n",
       "      <td>15.25</td>\n",
       "      <td>59.5</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0.51</td>\n",
       "      <td>KaggleStation</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-07 16:00:00</td>\n",
       "      <td>11.67</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17.43</td>\n",
       "      <td>0.46</td>\n",
       "      <td>KaggleStation</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp  PM2.5  PM10     O3    CO         Source  hour  \\\n",
       "0 2016-01-07 10:00:00  10.67  39.0  14.50  0.48  KaggleStation    10   \n",
       "1 2016-01-07 11:00:00   2.00  39.0  15.00  0.49  KaggleStation    11   \n",
       "2 2016-01-07 14:00:00  20.50  50.0  10.50  0.47  KaggleStation    14   \n",
       "3 2016-01-07 15:00:00  15.25  59.5   6.60  0.51  KaggleStation    15   \n",
       "4 2016-01-07 16:00:00  11.67  60.0  17.43  0.46  KaggleStation    16   \n",
       "\n",
       "   dayofweek  month  \n",
       "0          3      1  \n",
       "1          3      1  \n",
       "2          3      1  \n",
       "3          3      1  \n",
       "4          3      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fix Timestamp -> datetime, create time features, and show diagnostics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load dataset if not in memory\n",
    "try:\n",
    "    df  # if df exists, we use it\n",
    "except NameError:\n",
    "    df = pd.read_csv(\"data/master_airquality_clean.csv\", low_memory=False)\n",
    "\n",
    "# Find likely timestamp column(s)\n",
    "possible_ts = [c for c in df.columns if any(k in c.lower() for k in [\"time\",\"date\",\"timestamp\"])]\n",
    "print(\"Timestamp-like columns found:\", possible_ts)\n",
    "\n",
    "# If 'Timestamp' already present but not datetime, try to convert it.\n",
    "if 'Timestamp' in df.columns:\n",
    "    ts_col = 'Timestamp'\n",
    "else:\n",
    "    ts_col = possible_ts[0] if possible_ts else None\n",
    "\n",
    "if ts_col is None:\n",
    "    raise RuntimeError(\"No timestamp-like column found. Please tell me the column names: \" + \", \".join(df.columns))\n",
    "\n",
    "# Convert robustly (try several formats)\n",
    "df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
    "\n",
    "# If too many NaT, try alternate parsing (common alternative formats)\n",
    "nat_frac = df[ts_col].isna().mean()\n",
    "print(f\"After first parse, NaT fraction = {nat_frac:.3f}\")\n",
    "\n",
    "if nat_frac > 0.25:\n",
    "    # try parsing with no dayfirst\n",
    "    df[ts_col] = pd.to_datetime(df[ts_col].astype(str), errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    nat_frac2 = df[ts_col].isna().mean()\n",
    "    print(f\"After second parse (dayfirst=False), NaT fraction = {nat_frac2:.3f}\")\n",
    "\n",
    "# Rename unified column to 'Timestamp'\n",
    "df.rename(columns={ts_col: 'Timestamp'}, inplace=True)\n",
    "\n",
    "# Drop rows with missing Timestamp (can't use them for time features)\n",
    "n_before = len(df)\n",
    "df = df[~df['Timestamp'].isna()].copy()\n",
    "n_after = len(df)\n",
    "print(f\"Dropped {n_before - n_after} rows with unparseable Timestamp\")\n",
    "\n",
    "# Create time features\n",
    "df['hour'] = df['Timestamp'].dt.hour\n",
    "df['dayofweek'] = df['Timestamp'].dt.dayofweek\n",
    "df['month'] = df['Timestamp'].dt.month\n",
    "\n",
    "# Ensure numeric pollutant columns exist\n",
    "for col in ['PM2.5','PM10','O3','CO']:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "    else:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Quick diagnostics\n",
    "print(\"Dataset now has shape:\", df.shape)\n",
    "print(\"Timestamp min/max:\", df['Timestamp'].min(), \"/\", df['Timestamp'].max())\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcea4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure features list exists\n",
    "features = ['PM10', 'O3', 'CO', 'hour', 'dayofweek', 'month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2d11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3813205, 6) Test: (953301, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n = len(df)\n",
    "test_size = int(0.2 * n)\n",
    "train_df = df.iloc[:n - test_size].copy()\n",
    "test_df  = df.iloc[n - test_size:].copy()\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train = train_df['PM2.5']\n",
    "X_test  = test_df[features]\n",
    "y_test  = test_df['PM2.5']\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b5eca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Missing values after imputation:\n",
      "Train: 0 Test: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Impute (fill) missing values in features\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=features)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=features)\n",
    "\n",
    "# Just to confirm:\n",
    "print(\"✅ Missing values after imputation:\")\n",
    "print(\"Train:\", np.isnan(X_train.values).sum(), \"Test:\", np.isnan(X_test.values).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7b668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression -> RMSE: 40.1835, R2: 0.6015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest -> RMSE: 39.7948, R2: 0.6091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost -> RMSE: 40.0060, R2: 0.6050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'linear': {'rmse': 40.183530025414086, 'r2': 0.6014529977703444},\n",
       " 'rf': {'rmse': 39.794836109028864, 'r2': 0.6091259703523371},\n",
       " 'xgb': {'rmse': 40.00601876637627, 'r2': 0.604966393340345}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "def eval_model(name, model, X_test, y_test):\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"{name} -> RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "    return {\"rmse\": rmse, \"r2\": r2}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "results['linear'] = eval_model(\"LinearRegression\", lr, X_test, y_test)\n",
    "joblib.dump(lr, \"models/linear_reg.joblib\")\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import gc\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,       # fewer trees (cut memory ~½)\n",
    "    max_depth=20,           # limit depth to control tree size\n",
    "    n_jobs=-1,              # still use all cores\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict(X_test)\n",
    "results['rf'] = eval_model(\"RandomForest\", rf, X_test, y_test)\n",
    "joblib.dump(rf, \"models/rf_reg.joblib\")\n",
    "\n",
    "gc.collect()   # free memory\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efe4d3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost -> RMSE: 40.0060, R2: 0.6050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'linear': {'rmse': 40.183530025414086, 'r2': 0.6014529977703444},\n",
       " 'rf': {'rmse': 39.794836109028864, 'r2': 0.6091259703523371},\n",
       " 'xgb': {'rmse': 40.00601876637627, 'r2': 0.604966393340345}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# XGBoost\n",
    "xgr = xgb.XGBRegressor(n_estimators=300, tree_method='hist', random_state=42, verbosity=0)\n",
    "xgr.fit(X_train, y_train)\n",
    "results['xgb'] = eval_model(\"XGBoost\", xgr, X_test, y_test)\n",
    "xgr.save_model(\"models/xgb_reg.json\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e71a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.20.*\n",
      "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.33.0\n",
      "    Uninstalling protobuf-6.33.0:\n",
      "      Successfully uninstalled protobuf-6.33.0\n",
      "Successfully installed protobuf-3.20.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "databricks-sdk 0.73.0 requires protobuf!=5.26.*,!=5.27.*,!=5.28.*,!=5.29.0,!=5.29.1,!=5.29.2,!=5.29.3,!=5.29.4,!=6.30.0,!=6.30.1,!=6.31.0,<7.0,>=4.25.8, but you have protobuf 3.20.3 which is incompatible.\n",
      "opentelemetry-proto 1.38.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691cf1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vineet Raj\\AppData\\Local\\Temp\\ipykernel_23544\\3259342412.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  reg_mae_source = df_all.groupby('Source').apply(lambda g: mean_absolute_error(g['PM2.5'], g['pred'])).to_dict()\n",
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved governance_report.json\n",
      "{'regression_mae_by_source': {'CityCombined': 16.681804290273604, 'KaggleStation': 21.94193433674848}, 'sample_top_cities_mae': {}, 'aif360_weights_sample': [0.9393110488990309, 0.9999344632685842, 1.0000320881265077, 1.1520370612025757]}\n"
     ]
    }
   ],
   "source": [
    "# governance_check.py (run in notebook cell or save as file and run)\n",
    "import pandas as pd\n",
    "import numpy as np, json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "# load data and model\n",
    "df_all = df.copy()   # your cleaned dataframe in memory\n",
    "model = joblib.load(\"models/rf_reg.joblib\")  # or use best model\n",
    "\n",
    "# Regression fairness: MAE by Source and (if available) by City\n",
    "df_all['pred'] = model.predict(df_all[features].fillna(df_all[features].median()))\n",
    "reg_mae_source = df_all.groupby('Source').apply(lambda g: mean_absolute_error(g['PM2.5'], g['pred'])).to_dict()\n",
    "reg_mae_city = df_all.groupby('City').apply(lambda g: mean_absolute_error(g['PM2.5'], g['pred'])) if 'City' in df_all.columns else None\n",
    "\n",
    "report = {\n",
    "    \"regression_mae_by_source\": reg_mae_source,\n",
    "    \"sample_top_cities_mae\": (reg_mae_city.sort_values(ascending=False).head(10).to_dict() if reg_mae_city is not None else {})\n",
    "}\n",
    "\n",
    "# AIF360 classification proxy (High vs NotHigh) + reweighing\n",
    "try:\n",
    "    from aif360.datasets import BinaryLabelDataset\n",
    "    from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "    df_clf = df_all.copy()\n",
    "    df_clf['label_high'] = (df_clf['PM2.5'] >= 60).astype(int)\n",
    "    df_clf['protected'] = (df_clf['Source'] == 'CityCombined').astype(int)\n",
    "    X = df_clf[features].fillna(df_clf[features].median())\n",
    "    data_for_aif = pd.DataFrame(np.hstack([X.values, df_clf['label_high'].values.reshape(-1,1), df_clf['protected'].values.reshape(-1,1)]),\n",
    "                                columns = [*features,'label','protected'])\n",
    "    dataset = BinaryLabelDataset(df=data_for_aif, label_names=['label'], protected_attribute_names=['protected'])\n",
    "    rw = Reweighing(unprivileged_groups=[{'protected':0}], privileged_groups=[{'protected':1}])\n",
    "    dataset_transf = rw.fit_transform(dataset)\n",
    "    # show weight summary\n",
    "    unique_weights = np.unique(dataset_transf.instance_weights)[:10].tolist()\n",
    "    report['aif360_weights_sample'] = unique_weights\n",
    "except Exception as e:\n",
    "    report['aif360_error'] = str(e)\n",
    "\n",
    "open(\"governance_report.json\",\"w\").write(json.dumps(report, indent=2))\n",
    "print(\"Saved governance_report.json\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
