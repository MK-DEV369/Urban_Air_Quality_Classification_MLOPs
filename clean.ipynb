{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d0a1f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 CSV files.\n",
      "[1/11] Processed bengaluru_combined.csv (1827 rows)\n",
      "[2/11] Processed chennai_combined.csv (1827 rows)\n",
      "[3/11] Processed delhi_combined.csv (1827 rows)\n",
      "[4/11] Processed gwalior_combined.csv (1827 rows)\n",
      "[5/11] Processed hyderabad_combined.csv (1827 rows)\n",
      "[6/11] Processed jaipur_combined.csv (1827 rows)\n",
      "[7/11] Processed kolkata_combined.csv (1827 rows)\n",
      "[8/11] Processed lucknow_combined.csv (1827 rows)\n",
      "[9/11] Processed mumbai_combined.csv (1827 rows)\n",
      "⚠️ Skipped data/stations_csvs\\stations_info.csv (no relevant columns found)\n",
      "[11/11] Processed visakhapatnam_combined.csv (1827 rows)\n",
      "\n",
      "✅ Done! Fixed merged file saved to: data/stations_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def load_pm_pollutants_fixed(dir_path=\"data/stations_csvs\", out_file=\"data/stations_combined.csv\"):\n",
    "    files = glob.glob(os.path.join(dir_path, \"*.csv\"))\n",
    "    print(f\"Found {len(files)} CSV files.\")\n",
    "    os.makedirs(os.path.dirname(out_file), exist_ok=True)\n",
    "\n",
    "    first = True\n",
    "    for i, f in enumerate(files, 1):\n",
    "        try:\n",
    "            # Read column headers first\n",
    "            cols = [c.strip() for c in pd.read_csv(f, nrows=0).columns]\n",
    "\n",
    "            # Detect possible matching column names\n",
    "            mapping = {}\n",
    "            for c in cols:\n",
    "                c_lower = c.lower()\n",
    "                if \"date\" in c_lower or \"time\" in c_lower:\n",
    "                    mapping[c] = \"Timestamp\"\n",
    "                elif \"pm2\" in c_lower:\n",
    "                    mapping[c] = \"PM2.5\"\n",
    "                elif \"pm10\" in c_lower:\n",
    "                    mapping[c] = \"PM10\"\n",
    "                elif \"o3\" in c_lower or \"ozone\" in c_lower:\n",
    "                    mapping[c] = \"O3\"\n",
    "                elif \"co\" in c_lower and not \"co2\" in c_lower:\n",
    "                    mapping[c] = \"CO\"\n",
    "\n",
    "            if not mapping:\n",
    "                print(f\"⚠️ Skipped {f} (no relevant columns found)\")\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(f, usecols=mapping.keys(), encoding_errors=\"ignore\")\n",
    "            df.rename(columns=mapping, inplace=True)\n",
    "            df[\"StationFile\"] = os.path.basename(f)\n",
    "\n",
    "            # Make sure all columns exist\n",
    "            for col in [\"Timestamp\", \"PM2.5\", \"PM10\", \"O3\", \"CO\"]:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = None\n",
    "\n",
    "            if first:\n",
    "                df.to_csv(out_file, index=False)\n",
    "                first = False\n",
    "            else:\n",
    "                df.to_csv(out_file, mode=\"a\", index=False, header=False)\n",
    "\n",
    "            print(f\"[{i}/{len(files)}] Processed {os.path.basename(f)} ({len(df)} rows)\")\n",
    "            del df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Skipped {f}: {e}\")\n",
    "\n",
    "    print(f\"\\n✅ Done! Fixed merged file saved to: {out_file}\")\n",
    "\n",
    "# Run this to rebuild Kaggle merged dataset\n",
    "load_pm_pollutants_fixed(\"data/stations_csvs\", \"data/stations_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468685aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>StationFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-01-2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bengaluru_combined.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02-01-2020</td>\n",
       "      <td>43.67</td>\n",
       "      <td>134.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>21.82</td>\n",
       "      <td>bengaluru_combined.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03-01-2020</td>\n",
       "      <td>30.58</td>\n",
       "      <td>74.42</td>\n",
       "      <td>0.96</td>\n",
       "      <td>23.31</td>\n",
       "      <td>bengaluru_combined.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04-01-2020</td>\n",
       "      <td>66.35</td>\n",
       "      <td>155.68</td>\n",
       "      <td>2.54</td>\n",
       "      <td>29.70</td>\n",
       "      <td>bengaluru_combined.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05-01-2020</td>\n",
       "      <td>48.00</td>\n",
       "      <td>99.13</td>\n",
       "      <td>1.14</td>\n",
       "      <td>31.01</td>\n",
       "      <td>bengaluru_combined.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>06-01-2020</td>\n",
       "      <td>23.75</td>\n",
       "      <td>63.34</td>\n",
       "      <td>1.08</td>\n",
       "      <td>25.82</td>\n",
       "      <td>bengaluru_combined.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>07-01-2020</td>\n",
       "      <td>24.67</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>30.37</td>\n",
       "      <td>bengaluru_combined.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08-01-2020</td>\n",
       "      <td>34.18</td>\n",
       "      <td>79.06</td>\n",
       "      <td>0.99</td>\n",
       "      <td>29.61</td>\n",
       "      <td>bengaluru_combined.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>09-01-2020</td>\n",
       "      <td>41.61</td>\n",
       "      <td>98.00</td>\n",
       "      <td>1.11</td>\n",
       "      <td>30.97</td>\n",
       "      <td>bengaluru_combined.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10-01-2020</td>\n",
       "      <td>38.95</td>\n",
       "      <td>97.70</td>\n",
       "      <td>0.97</td>\n",
       "      <td>31.89</td>\n",
       "      <td>bengaluru_combined.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Timestamp  PM2.5    PM10    CO     O3             StationFile\n",
       "0  01-01-2020    NaN     NaN   NaN    NaN  bengaluru_combined.csv\n",
       "1  02-01-2020  43.67  134.00  0.91  21.82  bengaluru_combined.csv\n",
       "2  03-01-2020  30.58   74.42  0.96  23.31  bengaluru_combined.csv\n",
       "3  04-01-2020  66.35  155.68  2.54  29.70  bengaluru_combined.csv\n",
       "4  05-01-2020  48.00   99.13  1.14  31.01  bengaluru_combined.csv\n",
       "5  06-01-2020  23.75   63.34  1.08  25.82  bengaluru_combined.csv\n",
       "6  07-01-2020  24.67   72.00  0.98  30.37  bengaluru_combined.csv\n",
       "7  08-01-2020  34.18   79.06  0.99  29.61  bengaluru_combined.csv\n",
       "8  09-01-2020  41.61   98.00  1.11  30.97  bengaluru_combined.csv\n",
       "9  10-01-2020  38.95   97.70  0.97  31.89  bengaluru_combined.csv"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_kaggle = pd.read_csv(\"data/stations_combined.csv\", on_bad_lines=\"skip\", low_memory=False)\n",
    "merged_kaggle.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c207be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_22016\\995085232.py:1: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  city_df = pd.read_csv(\"data/cities_combined.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined dataset shape: (14313933, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>43.67</td>\n",
       "      <td>134.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>0.91</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>30.58</td>\n",
       "      <td>74.42</td>\n",
       "      <td>23.31</td>\n",
       "      <td>0.96</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>66.35</td>\n",
       "      <td>155.68</td>\n",
       "      <td>29.7</td>\n",
       "      <td>2.54</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>48.00</td>\n",
       "      <td>99.13</td>\n",
       "      <td>31.01</td>\n",
       "      <td>1.14</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Timestamp  PM2.5    PM10     O3    CO         Source\n",
       "0 2020-01-01    NaN     NaN    NaN   NaN  KaggleStation\n",
       "1 2020-02-01  43.67  134.00  21.82  0.91  KaggleStation\n",
       "2 2020-03-01  30.58   74.42  23.31  0.96  KaggleStation\n",
       "3 2020-04-01  66.35  155.68   29.7  2.54  KaggleStation\n",
       "4 2020-05-01  48.00   99.13  31.01  1.14  KaggleStation"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_df = pd.read_csv(\"data/cities_combined.csv\")\n",
    "\n",
    "# Make sure Timestamp format matches\n",
    "for df in [merged_kaggle, city_df]:\n",
    "    for col in df.columns:\n",
    "        if \"time\" in col.lower() or \"date\" in col.lower():\n",
    "            df.rename(columns={col: \"Timestamp\"}, inplace=True)\n",
    "            break\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "merged_kaggle[\"Source\"] = \"KaggleStation\"\n",
    "city_df[\"Source\"] = \"CityCombined\"\n",
    "\n",
    "keep_cols = [\"Timestamp\", \"PM2.5\", \"PM10\", \"O3\", \"CO\", \"Source\"]\n",
    "for col in keep_cols:\n",
    "    if col not in city_df.columns:\n",
    "        city_df[col] = None\n",
    "\n",
    "combined_df = pd.concat([merged_kaggle[keep_cols], city_df[keep_cols]], ignore_index=True)\n",
    "print(\"✅ Combined dataset shape:\", combined_df.shape)\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7e96f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_22016\\658508099.py:7: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  city_df = pd.read_csv(\"data/cities_combined.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle dataset shape: (18270, 6)\n",
      "City dataset shape: (14295663, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>StationFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-01-2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bengaluru_combined.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02-01-2020</td>\n",
       "      <td>43.67</td>\n",
       "      <td>134.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>21.82</td>\n",
       "      <td>bengaluru_combined.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03-01-2020</td>\n",
       "      <td>30.58</td>\n",
       "      <td>74.42</td>\n",
       "      <td>0.96</td>\n",
       "      <td>23.31</td>\n",
       "      <td>bengaluru_combined.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Timestamp  PM2.5    PM10    CO     O3             StationFile\n",
       "0  01-01-2020    NaN     NaN   NaN    NaN  bengaluru_combined.csv\n",
       "1  02-01-2020  43.67  134.00  0.91  21.82  bengaluru_combined.csv\n",
       "2  03-01-2020  30.58   74.42  0.96  23.31  bengaluru_combined.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Timestamp.1</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>StationFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 10:00:00</td>\n",
       "      <td>2016-07-01 11:00:00</td>\n",
       "      <td>10.67</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>14.5</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 11:00:00</td>\n",
       "      <td>2016-07-01 12:00:00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>15.0</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 12:00:00</td>\n",
       "      <td>2016-07-01 13:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AP001.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Timestamp          Timestamp.1  PM2.5  PM10    CO    O3  \\\n",
       "0  2016-07-01 10:00:00  2016-07-01 11:00:00  10.67  39.0  0.48  14.5   \n",
       "1  2016-07-01 11:00:00  2016-07-01 12:00:00   2.00  39.0  0.49  15.0   \n",
       "2  2016-07-01 12:00:00  2016-07-01 13:00:00    NaN   NaN   NaN   NaN   \n",
       "\n",
       "  StationFile  \n",
       "0   AP001.csv  \n",
       "1   AP001.csv  \n",
       "2   AP001.csv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new Kaggle dataset (the fixed one)\n",
    "kaggle_df = pd.read_csv(\"data/stations_combined.csv\", on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "# Load your Combined City Pollution dataset\n",
    "city_df = pd.read_csv(\"data/cities_combined.csv\")\n",
    "\n",
    "print(\"Kaggle dataset shape:\", kaggle_df.shape)\n",
    "print(\"City dataset shape:\", city_df.shape)\n",
    "\n",
    "# Show first few rows from both\n",
    "display(kaggle_df.head(3))\n",
    "display(city_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16fd0286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined dataset shape: (14302863, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>43.67</td>\n",
       "      <td>134.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>0.91</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>30.58</td>\n",
       "      <td>74.42</td>\n",
       "      <td>23.31</td>\n",
       "      <td>0.96</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>66.35</td>\n",
       "      <td>155.68</td>\n",
       "      <td>29.7</td>\n",
       "      <td>2.54</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>48.00</td>\n",
       "      <td>99.13</td>\n",
       "      <td>31.01</td>\n",
       "      <td>1.14</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Timestamp  PM2.5    PM10     O3    CO         Source\n",
       "0 2020-01-01    NaN     NaN    NaN   NaN  KaggleStation\n",
       "1 2020-02-01  43.67  134.00  21.82  0.91  KaggleStation\n",
       "2 2020-03-01  30.58   74.42  23.31  0.96  KaggleStation\n",
       "3 2020-04-01  66.35  155.68   29.7  2.54  KaggleStation\n",
       "4 2020-05-01  48.00   99.13  31.01  1.14  KaggleStation"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize timestamp names\n",
    "for df in [kaggle_df, city_df]:\n",
    "    for col in df.columns:\n",
    "        if \"time\" in col.lower() or \"date\" in col.lower():\n",
    "            df.rename(columns={col: \"Timestamp\"}, inplace=True)\n",
    "            break\n",
    "\n",
    "# Force datetime conversion + DROP invalid timestamps\n",
    "for df in [kaggle_df, city_df]:\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "    df.dropna(subset=[\"Timestamp\"], inplace=True)\n",
    "\n",
    "# Add source column\n",
    "kaggle_df[\"Source\"] = \"KaggleStation\"\n",
    "city_df[\"Source\"] = \"CityCombined\"\n",
    "\n",
    "# Keep only the matching pollutant columns\n",
    "keep_cols = [\"Timestamp\", \"PM2.5\", \"PM10\", \"O3\", \"CO\", \"Source\"]\n",
    "\n",
    "# Add missing columns\n",
    "for col in keep_cols:\n",
    "    if col not in city_df.columns:\n",
    "        city_df[col] = None\n",
    "\n",
    "# Merge both datasets\n",
    "combined_df = pd.concat(\n",
    "    [kaggle_df[keep_cols], city_df[keep_cols]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(\"✅ Combined dataset shape:\", combined_df.shape)\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b70e7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp          0\n",
      "PM2.5        3829690\n",
      "PM10         4422984\n",
      "O3           3294466\n",
      "CO           3267519\n",
      "Source             0\n",
      "dtype: int64\n",
      "✅ After cleaning: (12134002, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>O3</th>\n",
       "      <th>CO</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>43.67</td>\n",
       "      <td>134.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>0.91</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>30.58</td>\n",
       "      <td>74.42</td>\n",
       "      <td>23.31</td>\n",
       "      <td>0.96</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>66.35</td>\n",
       "      <td>155.68</td>\n",
       "      <td>29.7</td>\n",
       "      <td>2.54</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>48.00</td>\n",
       "      <td>99.13</td>\n",
       "      <td>31.01</td>\n",
       "      <td>1.14</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>23.75</td>\n",
       "      <td>63.34</td>\n",
       "      <td>25.82</td>\n",
       "      <td>1.08</td>\n",
       "      <td>KaggleStation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Timestamp  PM2.5    PM10     O3    CO         Source\n",
       "1 2020-02-01  43.67  134.00  21.82  0.91  KaggleStation\n",
       "2 2020-03-01  30.58   74.42  23.31  0.96  KaggleStation\n",
       "3 2020-04-01  66.35  155.68   29.7  2.54  KaggleStation\n",
       "4 2020-05-01  48.00   99.13  31.01  1.14  KaggleStation\n",
       "5 2020-06-01  23.75   63.34  25.82  1.08  KaggleStation"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values\n",
    "print(combined_df.isna().sum())\n",
    "\n",
    "# Drop rows with completely empty pollutant data\n",
    "combined_df = combined_df.dropna(subset=[\"PM2.5\", \"PM10\", \"O3\", \"CO\"], how=\"all\")\n",
    "\n",
    "# Fill remaining missing values with median (optional)\n",
    "combined_df[[\"PM2.5\", \"PM10\", \"O3\", \"CO\"]] = combined_df[[\"PM2.5\", \"PM10\", \"O3\", \"CO\"]].fillna(\n",
    "    combined_df.median(numeric_only=True)\n",
    ")\n",
    "\n",
    "print(\"✅ After cleaning:\", combined_df.shape)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0d7631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved clean dataset: data/master_airquality_clean.csv\n"
     ]
    }
   ],
   "source": [
    "combined_df.to_csv(\"data/master_airquality_clean.csv\", index=False)\n",
    "print(\"✅ Saved clean dataset: data/master_airquality_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b0ed5",
   "metadata": {},
   "source": [
    "#IMPORTANT PROJECT INSTALLATION DETAILS AFTER THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a894b1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: scikit-learn in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: xgboost in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: joblib in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: matplotlib in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in e:\\5th sem data\\ai254ta-machine learning operations(mlops)\\mlops_project\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "# pip install (run in a notebook cell with `!` or in terminal)\n",
    "!pip install --upgrade pip # python -m pip install --upgrade pip\n",
    "!pip install pandas numpy scikit-learn xgboost joblib matplotlib seaborn\n",
    "!pip install aif360 metaflow wandb bentoml whylabs-sdk fastparquet # pip install whylogs whylabs-client whylabs-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36336576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_22016\\4248515241.py:25: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce', dayfirst=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp-like columns found: ['Timestamp', 'Timestamp.1']\n",
      "After first parse, NaT fraction = 0.000\n",
      "Dropped 0 rows with unparseable Timestamp\n",
      "Dataset now has shape: (14295663, 11)\n",
      "Timestamp min/max: 2010-01-01 00:00:00 / 2023-03-31 23:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Timestamp.1</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>StationFile</th>\n",
       "      <th>Source</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-01 10:00:00</td>\n",
       "      <td>2016-07-01 11:00:00</td>\n",
       "      <td>10.67</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>14.5</td>\n",
       "      <td>AP001.csv</td>\n",
       "      <td>CityCombined</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-01 11:00:00</td>\n",
       "      <td>2016-07-01 12:00:00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>15.0</td>\n",
       "      <td>AP001.csv</td>\n",
       "      <td>CityCombined</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-01 12:00:00</td>\n",
       "      <td>2016-07-01 13:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AP001.csv</td>\n",
       "      <td>CityCombined</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-07-01 13:00:00</td>\n",
       "      <td>2016-07-01 14:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AP001.csv</td>\n",
       "      <td>CityCombined</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-07-01 14:00:00</td>\n",
       "      <td>2016-07-01 15:00:00</td>\n",
       "      <td>20.50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.5</td>\n",
       "      <td>AP001.csv</td>\n",
       "      <td>CityCombined</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Timestamp          Timestamp.1  PM2.5  PM10    CO    O3  \\\n",
       "0 2016-07-01 10:00:00  2016-07-01 11:00:00  10.67  39.0  0.48  14.5   \n",
       "1 2016-07-01 11:00:00  2016-07-01 12:00:00   2.00  39.0  0.49  15.0   \n",
       "2 2016-07-01 12:00:00  2016-07-01 13:00:00    NaN   NaN   NaN   NaN   \n",
       "3 2016-07-01 13:00:00  2016-07-01 14:00:00    NaN   NaN   NaN   NaN   \n",
       "4 2016-07-01 14:00:00  2016-07-01 15:00:00  20.50  50.0  0.47  10.5   \n",
       "\n",
       "  StationFile        Source  hour  dayofweek  month  \n",
       "0   AP001.csv  CityCombined    10          4      7  \n",
       "1   AP001.csv  CityCombined    11          4      7  \n",
       "2   AP001.csv  CityCombined    12          4      7  \n",
       "3   AP001.csv  CityCombined    13          4      7  \n",
       "4   AP001.csv  CityCombined    14          4      7  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fix Timestamp -> datetime, create time features, and show diagnostics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load dataset if not in memory\n",
    "try:\n",
    "    df  # if df exists, we use it\n",
    "except NameError:\n",
    "    df = pd.read_csv(\"data/master_airquality_clean.csv\", low_memory=False)\n",
    "\n",
    "# Find likely timestamp column(s)\n",
    "possible_ts = [c for c in df.columns if any(k in c.lower() for k in [\"time\",\"date\",\"timestamp\"])]\n",
    "print(\"Timestamp-like columns found:\", possible_ts)\n",
    "\n",
    "# If 'Timestamp' already present but not datetime, try to convert it.\n",
    "if 'Timestamp' in df.columns:\n",
    "    ts_col = 'Timestamp'\n",
    "else:\n",
    "    ts_col = possible_ts[0] if possible_ts else None\n",
    "\n",
    "if ts_col is None:\n",
    "    raise RuntimeError(\"No timestamp-like column found. Please tell me the column names: \" + \", \".join(df.columns))\n",
    "\n",
    "# Convert robustly (try several formats)\n",
    "df[ts_col] = pd.to_datetime(df[ts_col], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
    "\n",
    "# If too many NaT, try alternate parsing (common alternative formats)\n",
    "nat_frac = df[ts_col].isna().mean()\n",
    "print(f\"After first parse, NaT fraction = {nat_frac:.3f}\")\n",
    "\n",
    "if nat_frac > 0.25:\n",
    "    # try parsing with no dayfirst\n",
    "    df[ts_col] = pd.to_datetime(df[ts_col].astype(str), errors='coerce', dayfirst=False, infer_datetime_format=True)\n",
    "    nat_frac2 = df[ts_col].isna().mean()\n",
    "    print(f\"After second parse (dayfirst=False), NaT fraction = {nat_frac2:.3f}\")\n",
    "\n",
    "# Rename unified column to 'Timestamp'\n",
    "df.rename(columns={ts_col: 'Timestamp'}, inplace=True)\n",
    "\n",
    "# Drop rows with missing Timestamp (can't use them for time features)\n",
    "n_before = len(df)\n",
    "df = df[~df['Timestamp'].isna()].copy()\n",
    "n_after = len(df)\n",
    "print(f\"Dropped {n_before - n_after} rows with unparseable Timestamp\")\n",
    "\n",
    "# Create time features\n",
    "df['hour'] = df['Timestamp'].dt.hour\n",
    "df['dayofweek'] = df['Timestamp'].dt.dayofweek\n",
    "df['month'] = df['Timestamp'].dt.month\n",
    "\n",
    "# Ensure numeric pollutant columns exist\n",
    "for col in ['PM2.5','PM10','O3','CO']:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "    else:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Quick diagnostics\n",
    "print(\"Dataset now has shape:\", df.shape)\n",
    "print(\"Timestamp min/max:\", df['Timestamp'].min(), \"/\", df['Timestamp'].max())\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcea4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure features list exists\n",
    "features = ['PM10', 'O3', 'CO', 'hour', 'dayofweek', 'month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dc2d11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (11436531, 6) Test: (2859132, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n = len(df)\n",
    "test_size = int(0.2 * n)\n",
    "train_df = df.iloc[:n - test_size].copy()\n",
    "test_df  = df.iloc[n - test_size:].copy()\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train = train_df['PM2.5']\n",
    "X_test  = test_df[features]\n",
    "y_test  = test_df['PM2.5']\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76b5eca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Missing values after imputation:\n",
      "Train: 0 Test: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Impute (fill) missing values in features\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=features)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=features)\n",
    "\n",
    "# Just to confirm:\n",
    "print(\"✅ Missing values after imputation:\")\n",
    "print(\"Train:\", np.isnan(X_train.values).sum(), \"Test:\", np.isnan(X_test.values).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48d7b668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression -> RMSE: 7.2897, R2: 0.4220\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m\n\u001b[32m     33\u001b[39m rf = RandomForestRegressor(\n\u001b[32m     34\u001b[39m     n_estimators=\u001b[32m100\u001b[39m,       \u001b[38;5;66;03m# fewer trees (cut memory ~½)\u001b[39;00m\n\u001b[32m     35\u001b[39m     max_depth=\u001b[32m20\u001b[39m,           \u001b[38;5;66;03m# limit depth to control tree size\u001b[39;00m\n\u001b[32m     36\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m,              \u001b[38;5;66;03m# still use all cores\u001b[39;00m\n\u001b[32m     37\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m     38\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mrf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m pred_rf = rf.predict(X_test)\n\u001b[32m     41\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mrf\u001b[39m\u001b[33m'\u001b[39m] = eval_model(\u001b[33m\"\u001b[39m\u001b[33mRandomForest\u001b[39m\u001b[33m\"\u001b[39m, rf, X_test, y_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:486\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    475\u001b[39m trees = [\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    478\u001b[39m ]\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "import os\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def eval_model(name, model, X_test, y_test):\n",
    "    preds = model.predict(X_test)\n",
    "    mse = root_mean_squared_error(y_test, preds) # squared=False Parameter removed\n",
    "    rmse = mse**0.5  # Calculate RMSE manually\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"{name} -> RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "    return {\"rmse\": rmse, \"r2\": r2}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Linear Regression\n",
    "lr = LinearRegression()\n",
    "y_train = y_train.fillna(y_train.mean())\n",
    "y_test = y_test.fillna(y_test.mean())\n",
    "lr.fit(X_train, y_train)\n",
    "results['linear'] = eval_model(\"LinearRegression\", lr, X_test, y_test)\n",
    "joblib.dump(lr, \"models/linear_reg.joblib\")\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import gc\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,       # fewer trees (cut memory ~½)\n",
    "    max_depth=20,           # limit depth to control tree size\n",
    "    n_jobs=-1,              # still use all cores\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict(X_test)\n",
    "results['rf'] = eval_model(\"RandomForest\", rf, X_test, y_test)\n",
    "joblib.dump(rf, \"models/rf_reg.joblib\")\n",
    "\n",
    "gc.collect()   # free memory\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efe4d3ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# XGBoost\u001b[39;00m\n\u001b[32m      3\u001b[39m xgr = xgb.XGBRegressor(n_estimators=\u001b[32m300\u001b[39m, tree_method=\u001b[33m'\u001b[39m\u001b[33mhist\u001b[39m\u001b[33m'\u001b[39m, random_state=\u001b[32m42\u001b[39m, verbosity=\u001b[32m0\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mxgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m results[\u001b[33m'\u001b[39m\u001b[33mxgb\u001b[39m\u001b[33m'\u001b[39m] = eval_model(\u001b[33m\"\u001b[39m\u001b[33mXGBoost\u001b[39m\u001b[33m\"\u001b[39m, xgr, X_test, y_test)\n\u001b[32m      7\u001b[39m xgr.save_model(\u001b[33m\"\u001b[39m\u001b[33mmodels/xgb_reg.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\xgboost\\sklearn.py:1368\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1366\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\xgboost\\training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\xgboost\\core.py:2434\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2433\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2434\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2437\u001b[39m     )\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2439\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# XGBoost\n",
    "xgr = xgb.XGBRegressor(n_estimators=300, tree_method='hist', random_state=42, verbosity=0)\n",
    "\n",
    "xgr.fit(X_train, y_train)\n",
    "results['xgb'] = eval_model(\"XGBoost\", xgr, X_test, y_test)\n",
    "xgr.save_model(\"models/xgb_reg.json\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e71a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.20.*\n",
      "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "Successfully installed protobuf-3.20.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.18.0 requires opt-einsum>=2.3.2, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "691cf1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.6.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "e:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.6.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Regression fairness: MAE by Source and (if available) by City\u001b[39;00m\n\u001b[32m     12\u001b[39m df_all[\u001b[33m'\u001b[39m\u001b[33mpred\u001b[39m\u001b[33m'\u001b[39m] = model.predict(df_all[features].fillna(df_all[features].median()))\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m reg_mae_source = \u001b[43mdf_all\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSource\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPM2.5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpred\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.to_dict()\n\u001b[32m     14\u001b[39m reg_mae_city = df_all.groupby(\u001b[33m'\u001b[39m\u001b[33mCity\u001b[39m\u001b[33m'\u001b[39m).apply(\u001b[38;5;28;01mlambda\u001b[39;00m g: mean_absolute_error(g[\u001b[33m'\u001b[39m\u001b[33mPM2.5\u001b[39m\u001b[33m'\u001b[39m], g[\u001b[33m'\u001b[39m\u001b[33mpred\u001b[39m\u001b[33m'\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mCity\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df_all.columns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     16\u001b[39m report = {\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mregression_mae_by_source\u001b[39m\u001b[33m\"\u001b[39m: reg_mae_source,\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msample_top_cities_mae\u001b[39m\u001b[33m\"\u001b[39m: (reg_mae_city.sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).head(\u001b[32m10\u001b[39m).to_dict() \u001b[38;5;28;01mif\u001b[39;00m reg_mae_city \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m     19\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1826\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1824\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1825\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1826\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1827\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1828\u001b[39m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj, Series)\n\u001b[32m   1829\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1830\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selected_obj.shape != \u001b[38;5;28mself\u001b[39m._obj_with_exclusions.shape\n\u001b[32m   1831\u001b[39m         ):\n\u001b[32m   1832\u001b[39m             warnings.warn(\n\u001b[32m   1833\u001b[39m                 message=_apply_groupings_depr.format(\n\u001b[32m   1834\u001b[39m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1837\u001b[39m                 stacklevel=find_stack_level(),\n\u001b[32m   1838\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1887\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1852\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1853\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_python_apply_general\u001b[39m(\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1859\u001b[39m     is_agg: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1860\u001b[39m ) -> NDFrameT:\n\u001b[32m   1861\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1862\u001b[39m \u001b[33;03m    Apply function f in python space\u001b[39;00m\n\u001b[32m   1863\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1885\u001b[39m \u001b[33;03m        data after applying f\u001b[39;00m\n\u001b[32m   1886\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1887\u001b[39m     values, mutated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1888\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1889\u001b[39m         not_indexed_same = mutated\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:928\u001b[39m, in \u001b[36mBaseGrouper.apply_groupwise\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[32m    927\u001b[39m group_axes = group.axes\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m res = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[32m    930\u001b[39m     mutated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(g)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Regression fairness: MAE by Source and (if available) by City\u001b[39;00m\n\u001b[32m     12\u001b[39m df_all[\u001b[33m'\u001b[39m\u001b[33mpred\u001b[39m\u001b[33m'\u001b[39m] = model.predict(df_all[features].fillna(df_all[features].median()))\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m reg_mae_source = df_all.groupby(\u001b[33m'\u001b[39m\u001b[33mSource\u001b[39m\u001b[33m'\u001b[39m).apply(\u001b[38;5;28;01mlambda\u001b[39;00m g: \u001b[43mmean_absolute_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPM2.5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpred\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m).to_dict()\n\u001b[32m     14\u001b[39m reg_mae_city = df_all.groupby(\u001b[33m'\u001b[39m\u001b[33mCity\u001b[39m\u001b[33m'\u001b[39m).apply(\u001b[38;5;28;01mlambda\u001b[39;00m g: mean_absolute_error(g[\u001b[33m'\u001b[39m\u001b[33mPM2.5\u001b[39m\u001b[33m'\u001b[39m], g[\u001b[33m'\u001b[39m\u001b[33mpred\u001b[39m\u001b[33m'\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mCity\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df_all.columns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     16\u001b[39m report = {\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mregression_mae_by_source\u001b[39m\u001b[33m\"\u001b[39m: reg_mae_source,\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msample_top_cities_mae\u001b[39m\u001b[33m\"\u001b[39m: (reg_mae_city.sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).head(\u001b[32m10\u001b[39m).to_dict() \u001b[38;5;28;01mif\u001b[39;00m reg_mae_city \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m     19\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:284\u001b[39m, in \u001b[36mmean_absolute_error\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[32m    229\u001b[39m \n\u001b[32m    230\u001b[39m \u001b[33;03mThe mean absolute error is a non-negative floating point value, where best value\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    279\u001b[39m \u001b[33;03m0.85...\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    281\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[32m    283\u001b[39m _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m )\n\u001b[32m    289\u001b[39m output_errors = _average(\n\u001b[32m    290\u001b[39m     xp.abs(y_pred - y_true), weights=sample_weight, axis=\u001b[32m0\u001b[39m, xp=xp\n\u001b[32m    291\u001b[39m )\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(multioutput, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:209\u001b[39m, in \u001b[36m_check_reg_targets_with_floating_dtype\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Ensures y_true, y_pred, and sample_weight correspond to same regression task.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m    162\u001b[39m \u001b[33;03mExtends `_check_reg_targets` by automatically selecting a suitable floating-point\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    205\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    207\u001b[39m dtype_name = _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m y_type, y_true, y_pred, sample_weight, multioutput = \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y_type, y_true, y_pred, sample_weight, multioutput\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:115\u001b[39m, in \u001b[36m_check_reg_targets\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, dtype, xp)\u001b[39m\n\u001b[32m    112\u001b[39m xp, _ = get_namespace(y_true, y_pred, multioutput, xp=xp)\n\u001b[32m    114\u001b[39m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m y_true = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m y_pred = check_array(y_pred, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\5th SEM Data\\AI254TA-Machine Learning Operations(MLOps)\\MLOPs_Project\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# governance_check.py (run in notebook cell or save as file and run)\n",
    "import pandas as pd\n",
    "import numpy as np, json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "# load data and model\n",
    "df_all = df.copy()   # your cleaned dataframe in memory\n",
    "model = joblib.load(\"models/rf_reg.joblib\")  # or use best model\n",
    "\n",
    "# Regression fairness: MAE by Source and (if available) by City\n",
    "df_all['pred'] = model.predict(df_all[features].fillna(df_all[features].median()))\n",
    "reg_mae_source = df_all.groupby('Source').apply(lambda g: mean_absolute_error(g['PM2.5'], g['pred'])).to_dict()\n",
    "reg_mae_city = df_all.groupby('City').apply(lambda g: mean_absolute_error(g['PM2.5'], g['pred'])) if 'City' in df_all.columns else None\n",
    "\n",
    "report = {\n",
    "    \"regression_mae_by_source\": reg_mae_source,\n",
    "    \"sample_top_cities_mae\": (reg_mae_city.sort_values(ascending=False).head(10).to_dict() if reg_mae_city is not None else {})\n",
    "}\n",
    "\n",
    "# AIF360 classification proxy (High vs NotHigh) + reweighing\n",
    "try:\n",
    "    from aif360.datasets import BinaryLabelDataset\n",
    "    from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "    df_clf = df_all.copy()\n",
    "    df_clf['label_high'] = (df_clf['PM2.5'] >= 60).astype(int)\n",
    "    df_clf['protected'] = (df_clf['Source'] == 'CityCombined').astype(int)\n",
    "    X = df_clf[features].fillna(df_clf[features].median())\n",
    "    data_for_aif = pd.DataFrame(np.hstack([X.values, df_clf['label_high'].values.reshape(-1,1), df_clf['protected'].values.reshape(-1,1)]),\n",
    "                                columns = [*features,'label','protected'])\n",
    "    dataset = BinaryLabelDataset(df=data_for_aif, label_names=['label'], protected_attribute_names=['protected'])\n",
    "    rw = Reweighing(unprivileged_groups=[{'protected':0}], privileged_groups=[{'protected':1}])\n",
    "    dataset_transf = rw.fit_transform(dataset)\n",
    "    # show weight summary\n",
    "    unique_weights = np.unique(dataset_transf.instance_weights)[:10].tolist()\n",
    "    report['aif360_weights_sample'] = unique_weights\n",
    "except Exception as e:\n",
    "    report['aif360_error'] = str(e)\n",
    "\n",
    "open(\"governance_report.json\",\"w\").write(json.dumps(report, indent=2))\n",
    "print(\"Saved governance_report.json\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
